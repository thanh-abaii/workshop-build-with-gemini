{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrD2kWanydfP"
      },
      "source": [
        "##### Copyright 2025 Patrick Loeber"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wrgUJetgydfR"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si1uWsxtj0W6"
      },
      "source": [
        "# Workshop: Build with Gemini (Part 3)\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/patrickloeber/workshop-build-with-gemini/blob/main/notebooks/part-3-thinking-and-tools.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "This workshop teaches how to build with Gemini using the Gemini API and Python SDK.\n",
        "\n",
        "Course outline:\n",
        "\n",
        "- **[Part1: Quickstart + Text prompting](https://github.com/patrickloeber/workshop-build-with-gemini/blob/main/notebooks/part-1-text-prompting.ipynb)**\n",
        "\n",
        "- **[Part 2: Multimodal understanding (image, video, audio, docs, code)](https://github.com/patrickloeber/workshop-build-with-gemini/blob/main/notebooks/part-2-multimodal-understanding.ipynb)**\n",
        "\n",
        "- **Part 3 (this notebook): Thinking models + agentic capabilities (tool usage)**\n",
        "  - Thinking models\n",
        "  - Structured outputps\n",
        "  - Code execution\n",
        "  - Grounding with Google Search\n",
        "  - Function calling\n",
        "  - Final excercise: Give Gemini access to the Pok√©API to answer Pok√©mon questions\n",
        "\n",
        "## 0. Use the Google AI Studio as playground\n",
        "\n",
        "Explore and play with all models in the [Google AI Studio](https://aistudio.google.com/apikey).\n",
        "\n",
        "## 1. Setup\n",
        "\n",
        "Get a free API key in the [Google AI Studio](https://aistudio.google.com/apikey) and set up the [Google Gen AI Python SDK](https://github.com/googleapis/python-genai)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SzjZdf7mwD_",
        "outputId": "21c88fc8-2f89-40c0-8d82-0e4f2769d2c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/159.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ\u001b[0m \u001b[32m153.6/159.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ\u001b[0m \u001b[32m153.6/159.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m159.7/159.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -U -q google-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BF3gXZyFm3Pf"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0lajO_7dnFya"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drS_GiIih9kw"
      },
      "source": [
        "## Thinking models\n",
        "\n",
        "Starting with Gemini 2.5, all models have thinking capabilities. These models use an internal \"thinking process\" during response generation. This process contributes to their improved reasoning capabilities and allows them to solve complex tasks, particularly complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents.\n",
        "\n",
        "Thinking models are also great at working with tools to perform actions beyond generating text. This allows them to interact with external systems, execute code, or access real-time information, incorporating the results into their reasoning and final response.\n",
        "\n",
        "(Note: Tools are also available with Gemini 2.0 models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqCNX_91q2YN"
      },
      "outputs": [],
      "source": [
        "# 2.5 Pro\n",
        "MODEL = \"gemini-2.5-pro-exp-03-25\"  # with paid tier: gemini-2.5-pro-preview-03-25\n",
        "\n",
        "# 2.5 Flash\n",
        "# MODEL = \"gemini-2.5-flash-preview-04-17\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iRjV4L-sMxp",
        "outputId": "e8d17a05-7371-4e89-996f-a89a6c6367bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It still takes **5 minutes** to boil three eggs.\n",
            "\n",
            "You can boil them all at the same time in the same pot of water. The cooking time for each egg doesn't change based on how many others are in the pot (as long as the pot is big enough and the water stays boiling).\n"
          ]
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=\"If it takes 5 minutes to boil one egg, how long does it take to boil three eggs?\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCenTU9B0RiC"
      },
      "source": [
        "## **!! Exercise !!** ##\n",
        "\n",
        "- Go to [Google AI Studio](https://ai.dev/?model=gemini-2.5-pro-preview-03-25), use Gemini 2.5 Pro, give it a complex task, and pbserve the thinking process. For example, create a p5js game in one shot:\n",
        "\n",
        "```\n",
        "Make a p5js soccer game simulation. There should be 2 teams and each player on the team should have their path traveled displayed. Add live stats on the right side and score in the top bar. no HTML\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-qkPEabTURX"
      },
      "source": [
        "## Structured output\n",
        "\n",
        "Gemini generates unstructured text by default, but some applications require structured text. For these use cases, you can constrain Gemini to respond with JSON, a structured data format suitable for automated processing. You can also constrain the model to respond with one of the options specified in an enum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsnYkEF2Tcm8"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "  recipe_name: str\n",
        "  ingredients: list[str]\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents='List a three popular cookie recipes. Be sure to include the amounts of ingredients.',\n",
        "    config={\n",
        "        'response_mime_type': 'application/json',\n",
        "        'response_schema': list[Recipe],\n",
        "    },\n",
        ")\n",
        "# Use the response as a JSON string.\n",
        "print(response.text)\n",
        "\n",
        "# Use instantiated objects.\n",
        "my_recipes: list[Recipe] = response.parsed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp8wv9jstl96"
      },
      "source": [
        "Contrain to enums:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQbvMIvuLUUE",
        "outputId": "baa6da78-a46c-40c7-a3ae-967a5d2eac09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "froot\n"
          ]
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents='What type of food is a banana?',\n",
        "    config={\n",
        "        'response_mime_type': 'text/x.enum',\n",
        "        'response_schema': {\n",
        "            \"type\": \"STRING\",\n",
        "            \"enum\": [\"froot\", \"vegetable\", \"grains\", \"protein foods\", \"dairy\"],\n",
        "        },\n",
        "    },\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jicpPkAntnud"
      },
      "source": [
        "Or use the builtin Python enum class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0W_6JyZN_ED",
        "outputId": "1af704fc-41f6-4ac8-a078-7efe765ca5d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dairy\n"
          ]
        }
      ],
      "source": [
        "import enum\n",
        "\n",
        "class FOOD(enum.Enum):\n",
        "  FROOT = \"froot\"\n",
        "  VEGETABLE = \"vegetable\"\n",
        "  GRAINS = \"grains\"\n",
        "  PROTEIN_FOODS = \"protein foods\"\n",
        "  DAIRY = \"dairy\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents='What type of food is cheese?',\n",
        "    config={\n",
        "        'response_mime_type': 'text/x.enum',\n",
        "        'response_schema': FOOD,\n",
        "    },\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16C3AP4YOVlc"
      },
      "source": [
        "## Code execution\n",
        "\n",
        "The code execution feature enables the model to generate and run Python code and learn iteratively from the results until it arrives at a final output. You can use this code execution capability to build applications that benefit from code-based reasoning and that produce text output. For example, you could use code execution in an application that solves equations or processes text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SUMFt9wqOgVk"
      },
      "outputs": [],
      "source": [
        "from google.genai import types\n",
        "\n",
        "# In your prompt, give instruction to use/generate code\n",
        "\n",
        "response = client.models.generate_content(\n",
        "  model=MODEL,\n",
        "  contents='What is the sum of the first 50 prime numbers? '\n",
        "           'Generate and run code for the calculation.',\n",
        "  config=types.GenerateContentConfig(\n",
        "    tools=[types.Tool(\n",
        "      code_execution=types.ToolCodeExecution\n",
        "    )]\n",
        "  )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Bgfv66AOjiA",
        "outputId": "1abd3a15-94f8-45f7-c843-bae9af7ffda1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"Okay, I can help with that.\\n\\nHere's the plan:\\n1.  Define a function `is_prime(n)` to check if a given number `n` is prime.\\n2.  Initialize a counter for primes found and a variable for the sum.\\n3.  Iterate through numbers starting from 2.\\n4.  If a number is prime, add it to the sum and increment the prime counter.\\n5.  Stop when 50 primes have been found.\\n6.  Print the final sum.\\n\\nHere is the Python code to perform the calculation:\\n\"), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=ExecutableCode(code='import math\\n\\ndef is_prime(n):\\n    \"\"\"Checks if a number n is prime.\"\"\"\\n    if n <= 1:\\n        return False\\n    if n == 2:\\n        return True\\n    if n % 2 == 0:\\n        return False\\n    # Check odd divisors from 3 up to sqrt(n)\\n    for i in range(3, int(math.sqrt(n)) + 1, 2):\\n        if n % i == 0:\\n            return False\\n    return True\\n\\ncount = 0\\nnum = 2\\nprime_sum = 0\\n# Store the primes found for verification if needed\\nprimes_found = []\\n\\ntarget_count = 50\\n\\nwhile count < target_count:\\n    if is_prime(num):\\n        prime_sum += num\\n        primes_found.append(num)\\n        count += 1\\n    num += 1\\n\\n# print(f\"The first {target_count} prime numbers are: {primes_found}\")\\nprint(f\"The sum of the first {target_count} prime numbers is: {prime_sum}\")', language=<Language.PYTHON: 'PYTHON'>), file_data=None, function_call=None, function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=CodeExecutionResult(outcome=<Outcome.OUTCOME_OK: 'OUTCOME_OK'>, output='The sum of the first 50 prime numbers is: 5117\\n'), executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='**Findings:**\\n\\nBased on the executed code:\\nThe sum of the first 50 prime numbers is **5117**.')], role='model'), citation_metadata=None, finish_message=None, token_count=None, finish_reason=<FinishReason.STOP: 'STOP'>, avg_logprobs=None, grounding_metadata=None, index=0, logprobs_result=None, safety_ratings=None)], create_time=None, response_id=None, model_version='gemini-2.5-pro-exp-03-25', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cache_tokens_details=None, cached_content_token_count=None, candidates_token_count=1583, candidates_tokens_details=None, prompt_token_count=22, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=22)], thoughts_token_count=1437, tool_use_prompt_token_count=417, tool_use_prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=417)], total_token_count=2022, traffic_type=None), automatic_function_calling_history=[], parsed=None)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941
        },
        "id": "Yxo7JHSWOspM",
        "outputId": "9793fac5-c01a-4f65-dc76-d5536972384d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Okay, I can help with that.\n",
              "\n",
              "Here's the plan:\n",
              "1.  Define a function `is_prime(n)` to check if a given number `n` is prime.\n",
              "2.  Initialize a counter for primes found and a variable for the sum.\n",
              "3.  Iterate through numbers starting from 2.\n",
              "4.  If a number is prime, add it to the sum and increment the prime counter.\n",
              "5.  Stop when 50 primes have been found.\n",
              "6.  Print the final sum.\n",
              "\n",
              "Here is the Python code to perform the calculation:\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "---"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"background-color: #6a0cad;\">import math\n",
              "\n",
              "def is_prime(n):\n",
              "    \"\"\"Checks if a number n is prime.\"\"\"\n",
              "    if n <= 1:\n",
              "        return False\n",
              "    if n == 2:\n",
              "        return True\n",
              "    if n % 2 == 0:\n",
              "        return False\n",
              "    # Check odd divisors from 3 up to sqrt(n)\n",
              "    for i in range(3, int(math.sqrt(n)) + 1, 2):\n",
              "        if n % i == 0:\n",
              "            return False\n",
              "    return True\n",
              "\n",
              "count = 0\n",
              "num = 2\n",
              "prime_sum = 0\n",
              "# Store the primes found for verification if needed\n",
              "primes_found = []\n",
              "\n",
              "target_count = 50\n",
              "\n",
              "while count < target_count:\n",
              "    if is_prime(num):\n",
              "        prime_sum += num\n",
              "        primes_found.append(num)\n",
              "        count += 1\n",
              "    num += 1\n",
              "\n",
              "# print(f\"The first {target_count} prime numbers are: {primes_found}\")\n",
              "print(f\"The sum of the first {target_count} prime numbers is: {prime_sum}\")</pre>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "---"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "#### Output"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "The sum of the first 50 prime numbers is: 5117\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "---"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Findings:**\n",
              "\n",
              "Based on the executed code:\n",
              "The sum of the first 50 prime numbers is **5117**."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "---"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Image, Markdown, Code, HTML\n",
        "\n",
        "def display_code_execution_result(response):\n",
        "  for part in response.candidates[0].content.parts:\n",
        "    if part.text is not None:\n",
        "      display(Markdown(part.text))\n",
        "    if part.executable_code is not None:\n",
        "      code_html = f'<pre style=\"background-color: #6a0cad;\">{part.executable_code.code}</pre>' # Change code color\n",
        "      display(HTML(code_html))\n",
        "    if part.code_execution_result is not None:\n",
        "      display(Markdown(\"#### Output\"))\n",
        "      display(Markdown(part.code_execution_result.output))\n",
        "    if part.inline_data is not None:\n",
        "      display(Image(data=part.inline_data.data, format=\"png\"))\n",
        "    display(Markdown(\"---\"))\n",
        "\n",
        "display_code_execution_result(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0m5rasbQsDa"
      },
      "source": [
        "## Grounding with Google Search\n",
        "\n",
        "If Google Search is configured as a tool, Gemini can decide when to use Google Search to improve the accuracy and recency of responses.\n",
        "\n",
        "Here's a question about a recent event without Google Search:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "582GKc2DQ-N6",
        "outputId": "49dcb0f1-703a-4dc6-ac4d-b6f06d0b21cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Super Bowl in 2025 (Super Bowl LIX) hasn't happened yet!\n",
            "\n",
            "It is scheduled to be played on **February 9, 2025**, at the Caesars Superdome in New Orleans, Louisiana. It will determine the champion of the 2024 NFL season.\n",
            "\n",
            "We'll have to wait until then to find out who wins!\n"
          ]
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=\"Who won the super bowl in 2025?\",\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "SREuxqDSQs1y"
      },
      "outputs": [],
      "source": [
        "from google.genai.types import Tool, GenerateContentConfig, GoogleSearch\n",
        "\n",
        "google_search_tool = Tool(\n",
        "    google_search = GoogleSearch()\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=\"Who won the super bowl in 2025?\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[google_search_tool],\n",
        "        response_modalities=[\"TEXT\"],\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnCFGS7nQ9WB",
        "outputId": "f5e1f2ab-84fd-4925-9100-40281bd09447"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The **Philadelphia Eagles** won Super Bowl LIX in 2025.\n",
            "\n",
            "Here are some details about the game:\n",
            "\n",
            "*   **Date:** February 9, 2025\n",
            "*   **Location:** Caesars Superdome, New Orleans, Louisiana\n",
            "*   **Matchup:** Philadelphia Eagles (NFC Champion) vs. Kansas City Chiefs (AFC Champion and two-time defending Super Bowl champion)\n",
            "*   **Final Score:** Philadelphia Eagles 40, Kansas City Chiefs 22\n",
            "*   **Outcome:** The Eagles secured their second Super Bowl title in franchise history, preventing the Chiefs from achieving an unprecedented three consecutive Super Bowl wins.\n",
            "*   **MVP:** Eagles quarterback Jalen Hurts was named Super Bowl MVP. He threw for 221 yards and two touchdowns, and rushed for 72 yards and another touchdown.\n"
          ]
        }
      ],
      "source": [
        "for part in response.candidates[0].content.parts:\n",
        "    print(part.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "id": "BUgF_qSFQ6KW",
        "outputId": "0cfc43da-0b07-47ce-e46e-9e21839aad8c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>\n",
              ".container {\n",
              "  align-items: center;\n",
              "  border-radius: 8px;\n",
              "  display: flex;\n",
              "  font-family: Google Sans, Roboto, sans-serif;\n",
              "  font-size: 14px;\n",
              "  line-height: 20px;\n",
              "  padding: 8px 12px;\n",
              "}\n",
              ".chip {\n",
              "  display: inline-block;\n",
              "  border: solid 1px;\n",
              "  border-radius: 16px;\n",
              "  min-width: 14px;\n",
              "  padding: 5px 16px;\n",
              "  text-align: center;\n",
              "  user-select: none;\n",
              "  margin: 0 8px;\n",
              "  -webkit-tap-highlight-color: transparent;\n",
              "}\n",
              ".carousel {\n",
              "  overflow: auto;\n",
              "  scrollbar-width: none;\n",
              "  white-space: nowrap;\n",
              "  margin-right: -12px;\n",
              "}\n",
              ".headline {\n",
              "  display: flex;\n",
              "  margin-right: 4px;\n",
              "}\n",
              ".gradient-container {\n",
              "  position: relative;\n",
              "}\n",
              ".gradient {\n",
              "  position: absolute;\n",
              "  transform: translate(3px, -9px);\n",
              "  height: 36px;\n",
              "  width: 9px;\n",
              "}\n",
              "@media (prefers-color-scheme: light) {\n",
              "  .container {\n",
              "    background-color: #fafafa;\n",
              "    box-shadow: 0 0 0 1px #0000000f;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #1f1f1f;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #ffffff;\n",
              "    border-color: #d2d2d2;\n",
              "    color: #5e5e5e;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #d8d8d8;\n",
              "    border-color: #b6b6b6;\n",
              "  }\n",
              "  .logo-dark {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n",
              "  }\n",
              "}\n",
              "@media (prefers-color-scheme: dark) {\n",
              "  .container {\n",
              "    background-color: #1f1f1f;\n",
              "    box-shadow: 0 0 0 1px #ffffff26;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #fff;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #2c2c2c;\n",
              "    border-color: #3c4043;\n",
              "    color: #fff;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #464849;\n",
              "    border-color: #53575b;\n",
              "  }\n",
              "  .logo-light {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n",
              "  }\n",
              "}\n",
              "</style>\n",
              "<div class=\"container\">\n",
              "  <div class=\"headline\">\n",
              "    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\n",
              "      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\n",
              "      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\n",
              "      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\n",
              "      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\n",
              "  </div>\n",
              "  <div class=\"carousel\">\n",
              "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWQVqAIW5pBB8JWglgMm4Cu_QDriiuloRSjW7h6A_W_fHdpeVXOYlgD8WTZeoLEMLLlUXKe9uwDplPn6txwclo-vvb_OL5sy1vWNH0ff7Kj51y-Vc2ay7tgBPIYj269O3EjSYUYOk9vuEKVsNJ6zUTPHmB0KcoJaiMIHrBBx6HyjLPEtZoUbwCWPw9FJ8Krw5bR2p8Q1VA==\">Super Bowl 59 result</a>\n",
              "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWQVqALLr3_IEGFBL4dpxv7dPb5HGGIgsb8v9RIuFTQuSeLm5U2_PPYfcTrE6eg0_yviJ_Qqb5xQu0CQ_nZ20WlpEnKqjgq4YGR1me4wg6CJ6ohAzcozKdB6NhMdwfAKJzwU_sW7TeWmeHttVXP-Kngsr1eBgWE7IPi1Cjz334HLggoS5ZMeA9wGAJJJ9oZAxvojFWmmAjn-iIbnMg==\">Who won Super Bowl 2025?</a>\n",
              "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWQVqALD7pTFWUVv_h78o7c99aXBolh2MpoLuSa5oYIkh_oR_8SG9d2PlCUvqD7gRL9dyabkHcX6Frk-o4gzZG7sNK0qpscl9D_Ll64J0mokP72U4A8wHzlYRj-Ife3pRa_9ntibuVPnXZLGRKMdLcIhE26hPfk8qAn1tb0T29OLULS6Aa1jf3EpvjPbJOhXcEd0aWjv2Rg=\">Super Bowl LIX winner</a>\n",
              "  </div>\n",
              "</div>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# To get grounding metadata as web content.\n",
        "HTML(response.candidates[0].grounding_metadata.search_entry_point.rendered_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tN2AYpE2yqpQ"
      },
      "source": [
        "#### **!! Exercise !!**\n",
        "\n",
        "Use Gemini with Google Search for the current weather and the forecast for the next weekend in Berlin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRkMJFA6yoKt",
        "outputId": "aae6d370-5040-427c-b9ba-632a0438626e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the search results, here is the current weather and weekend forecast for Berlin:\n",
            "\n",
            "**Current Weather in Berlin:**\n",
            "\n",
            "*   The current temperature is around 11-15¬∞C.\n",
            "*   Conditions are currently cloudy or a mix of sun and clouds. Some sources mention possible showers, but generally dry.\n",
            "*   The temperature is expected to rise to a high of about 19¬∞C today.\n",
            "*   The wind is generally weak.\n",
            "\n",
            "**Weekend Weather Forecast for Berlin:**\n",
            "\n",
            "*   **Saturday:** The weather is expected to be lightly clouded or a mix of sun and clouds. Maximum temperatures are forecasted to be around 13¬∞C to 16¬∞C, with minimums around 3¬∞C to 9¬∞C. There is a low chance of precipitation.\n",
            "*   **Sunday:** Similar conditions to Saturday are expected, with partly cloudy skies or a mix of sun and clouds. Temperatures might be slightly warmer, with highs around 15¬∞C to 16¬∞C and lows around 4¬∞C to 5¬∞C. There is also a low chance of precipitation.\n",
            "\n",
            "Please note that weather forecasts can change, especially specific details like cloud cover and exact temperatures. The forecasts cited are based on information available around April 20th-21st for the following weekend (April 26th-27th).\n"
          ]
        }
      ],
      "source": [
        "from google.genai.types import Tool, GenerateContentConfig, GoogleSearch\n",
        "\n",
        "google_search_tool = Tool(\n",
        "    google_search = GoogleSearch()\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=\"Get the current weather in Berlin. Also get the forecast for the weekend\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[google_search_tool],\n",
        "        response_modalities=[\"TEXT\"],\n",
        "    )\n",
        ")\n",
        "\n",
        "for part in response.candidates[0].content.parts:\n",
        "    print(part.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKRcuZE_Rjl-"
      },
      "source": [
        "## Function calling\n",
        "\n",
        "Function calling lets you connect models to external tools and APIs. Instead of generating text responses, the model understands when to call specific functions and provides the necessary parameters to execute real-world actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "iL1FX3euRlQN"
      },
      "outputs": [],
      "source": [
        "from google.genai import types\n",
        "\n",
        "# Define the function declaration for the model\n",
        "weather_function = {\n",
        "    \"name\": \"get_current_temperature\",\n",
        "    \"description\": \"Gets the current temperature for a given location.\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"location\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The city name\",\n",
        "            },\n",
        "        },\n",
        "        \"required\": [\"location\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "# Configure the client and tools\n",
        "tools = types.Tool(function_declarations=[weather_function])\n",
        "\n",
        "# Send request with function declarations\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=\"What's the temperature in London?\",\n",
        "    config=types.GenerateContentConfig(tools=[tools])\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbWYwJ1G7s3_"
      },
      "source": [
        "Check for a function call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MvimiB5U30c",
        "outputId": "5540a599-6202-4bae-e4ec-0982eebf439c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Function to call: get_current_temperature\n",
            "Arguments: {'location': 'London'}\n"
          ]
        }
      ],
      "source": [
        "if response.candidates[0].content.parts[0].function_call:\n",
        "    function_call = response.candidates[0].content.parts[0].function_call\n",
        "    print(f\"Function to call: {function_call.name}\")\n",
        "    print(f\"Arguments: {function_call.args}\")\n",
        "    #  In a real app, you would call your function here:\n",
        "    #  result = get_current_temperature(**function_call.args)\n",
        "else:\n",
        "    print(\"No function call found in the response.\")\n",
        "    print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpV5sW9B0oBg"
      },
      "source": [
        "### Automatic Function Calling (Python Only)\n",
        "\n",
        "When using the Python SDK, you can provide Python functions directly as tools.\n",
        "\n",
        "The SDK handles the function call and returns the final text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqL-113f020c",
        "outputId": "bc0d8c7c-8272-404b-ab34-9bc8ad1f712d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The current temperature in Boston, USA is 25 degrees Celsius.\n"
          ]
        }
      ],
      "source": [
        "# Define the function with type hints and docstring\n",
        "def get_current_temperature(location: str) -> dict:\n",
        "    \"\"\"Gets the current temperature for a given location.\n",
        "\n",
        "    Args:\n",
        "        location: The city and country, e.g. San Francisco, USA\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the temperature and unit.\n",
        "    \"\"\"\n",
        "    # ... (implementation) ...\n",
        "    return {\"temperature\": 25, \"unit\": \"Celsius\"}\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=\"What's the temperature in Boston?\",\n",
        "    config=types.GenerateContentConfig(\n",
        "    tools=[get_current_temperature],\n",
        "    # to diable automatic funtion calling, you can set this:\n",
        "    # automatic_function_calling=types.AutomaticFunctionCallingConfig(disable=True)\n",
        "    )\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okP9B1cJ7yKi"
      },
      "source": [
        "Check the function calling history:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOQwksiK7z1X",
        "outputId": "1df3edab-a3e8-4248-f5de-8807ae2e124f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "id=None args={'location': 'Boston, USA'} name='get_current_temperature'\n"
          ]
        }
      ],
      "source": [
        "for content in response.automatic_function_calling_history:\n",
        "    for part in content.parts:\n",
        "        if part.function_call:\n",
        "            print(part.function_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfnxbheQ5GsO"
      },
      "source": [
        "## Exercise: Get Pok√©mon stats\n",
        "\n",
        "- Define a function that can work with the Pok√©API and get Pok√©mon stats.\n",
        "- Endpoint to use: `GET https://pokeapi.co/api/v2/pokemon/<pokekon_name>`\n",
        "- Call Gemini and give it access to the function, then answer questions like: `\"What stats does the Pokemon Squirtle have?\"`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNe1p_BkVOmu",
        "outputId": "b997e60e-213a-4315-f1e9-c10e00c33607"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Squirtle has the following base stats:\n",
            "*   **HP**: 44\n",
            "*   **Attack**: 48\n",
            "*   **Defense**: 65\n",
            "*   **Special Attack**: 50\n",
            "*   **Special Defense**: 64\n",
            "*   **Speed**: 43\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "def get_pokemon_info(pokemon: str) -> dict:\n",
        "    \"\"\"Gets pokemon info for a given pokemon name.\n",
        "\n",
        "    Args:\n",
        "        pokemon: The name of the pokemon.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the info.\n",
        "    \"\"\"\n",
        "    resp = requests.get(f\"https://pokeapi.co/api/v2/pokemon/{pokemon.lower()}\")\n",
        "    return resp.json()\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=\"What stats does the Pokemon Squirtle have?\",\n",
        "    config=types.GenerateContentConfig(tools=[get_pokemon_info])\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg1Bu_VgdARK",
        "outputId": "4ce2c6f6-3425-445d-f7ae-3d4c8d5b59c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "id=None args={'pokemon': 'Squirtle'} name='get_pokemon_info'\n"
          ]
        }
      ],
      "source": [
        "for content in response.automatic_function_calling_history:\n",
        "    for part in content.parts:\n",
        "        if part.function_call:\n",
        "            print(part.function_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTWLa3ZZ_yhT"
      },
      "source": [
        "## Recap & Next steps\n",
        "\n",
        "Awesome work! You learned about thinking models with advanced reasoning capabilities and how to combine Gemini with tools for agentic use cases.\n",
        "\n",
        "More helpful resources:\n",
        "\n",
        "- [Thinking docs](https://ai.google.dev/gemini-api/docs/thinking)\n",
        "- [Structured output docs](https://ai.google.dev/gemini-api/docs/structured-output?lang=python)\n",
        "- [Code execution docs](https://ai.google.dev/gemini-api/docs/code-execution?lang=python)\n",
        "- [Grounding docs](https://ai.google.dev/gemini-api/docs/grounding?lang=python)\n",
        "- [Function calling docs](https://ai.google.dev/gemini-api/docs/function-calling?example=weather)\n",
        "\n",
        "üéâüéâ**Conratulations, you completed the workshop!**üéâüéâ\n",
        "\n",
        "**Next steps**: There's even more you can do with Gemini which we didn't cover in this workshop:\n",
        "\n",
        "- [Image creation and editing with Gemini 2.0](https://github.com/patrickloeber/genai-tutorials/blob/main/notebooks/gemini-image-editing.ipynb)\n",
        "- [Live API: Talk to Gemini and share your camera](https://aistudio.google.com/live) & [Live API cookbook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.ipynb)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
